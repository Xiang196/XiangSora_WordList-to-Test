import streamlit as st
import json
import requests

API_URL = "https://api.deepseek.com/v1/chat/completions"
API_KEY = "aaa"

HEADERS = {
    "Content-Type": "application/json",
    "Authorization": f"Bearer {API_KEY}"
}

def call_deepseek_api(prompt, temperature=0.7):
    payload = {
        "model": "deepseek-chat",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": temperature
    }
    response = requests.post(API_URL, headers=HEADERS, json=payload)
    if response.status_code == 200:
        return response.json()["choices"][0]["message"]["content"]
    else:
        st.error(f"API è¯·æ±‚å¤±è´¥ï¼š{response.status_code}")
        return None
        
# ä¸å†éœ€è¦å¤æ‚çš„éšæœºåˆ†å¸ƒå‡½æ•°ï¼Œä½¿ç”¨ç®€å•çš„random.sample()å³å¯

def load_prompt(path):
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

def analyze_distribution(markdown_text, word_data=None):
    """åˆ†æè¯•å·ä¸­çš„é€‰é¡¹åˆ†å¸ƒå’Œå•è¯ä½¿ç”¨æƒ…å†µ"""
    import re
    from collections import Counter
    import pandas as pd
    import matplotlib.pyplot as plt
    import numpy as np
    
    # åˆ†æé€‰æ‹©é¢˜ç­”æ¡ˆåˆ†å¸ƒ
    answer_pattern = re.compile(r'\d+\.\s+([A-D])', re.MULTILINE)
    answers = answer_pattern.findall(markdown_text)
    
    # åˆ›å»ºé€‰é¡¹å¡
    tab1, tab2 = st.tabs(["ğŸ“Š é€‰é¡¹åˆ†å¸ƒ", "ğŸ“š å•è¯åˆ†å¸ƒ"])
    
    with tab1:
        if answers:
            # è®¡ç®—é€‰é¡¹åˆ†å¸ƒ
            answer_counts = Counter(answers)
            total = len(answers)
            
            # åˆ›å»ºé€‰é¡¹åˆ†å¸ƒè¡¨æ ¼
            option_data = {
                'é€‰é¡¹': list(answer_counts.keys()),
                'æ•°é‡': list(answer_counts.values()),
                'ç™¾åˆ†æ¯”': [f"{count/total*100:.1f}%" for count in answer_counts.values()]
            }
            
            st.subheader("ğŸ“Š é€‰é¡¹åˆ†å¸ƒåˆ†æ")
            
            # æ˜¾ç¤ºåˆ†å¸ƒè¡¨æ ¼
            st.dataframe(pd.DataFrame(option_data))
            
            # åˆ›å»ºé€‰é¡¹åˆ†å¸ƒå›¾è¡¨
            fig, ax = plt.subplots(figsize=(10, 6))
            bars = ax.bar(
                option_data['é€‰é¡¹'],
                [count/total*100 for count in answer_counts.values()],
                color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12']
            )
            
            # æ·»åŠ ç™¾åˆ†æ¯”æ ‡ç­¾
            for bar in bars:
                height = bar.get_height()
                ax.text(
                    bar.get_x() + bar.get_width()/2.,
                    height + 0.5,
                    f'{height:.1f}%',
                    ha='center', 
                    va='bottom'
                )
            
            # æ·»åŠ ç†æƒ³åˆ†å¸ƒå‚è€ƒçº¿
            ax.axhline(y=25, color='r', linestyle='--', alpha=0.3)
            ax.text(0, 26, 'ç†æƒ³åˆ†å¸ƒ (25%)', color='r', alpha=0.7)
            
            # è®¾ç½®å›¾è¡¨æ ·å¼
            ax.set_ylim(0, max([count/total*100 for count in answer_counts.values()]) + 5)
            ax.set_ylabel('ç™¾åˆ†æ¯” (%)')
            ax.set_title('é€‰æ‹©é¢˜ç­”æ¡ˆåˆ†å¸ƒ')
            ax.grid(axis='y', alpha=0.3)
            
            # æ˜¾ç¤ºå›¾è¡¨
            st.pyplot(fig)
            
            # åˆ†æç»“æœ
            max_deviation = max([abs(count/total*100 - 25) for count in answer_counts.values()])
            if max_deviation <= 2:
                st.success("âœ… é€‰é¡¹åˆ†å¸ƒéå¸¸å‡åŒ€ï¼Œæœ€å¤§åå·®å°äº2%")
            elif max_deviation <= 5:
                st.info("â„¹ï¸ é€‰é¡¹åˆ†å¸ƒåŸºæœ¬å‡åŒ€ï¼Œæœ€å¤§åå·®å°äº5%")
            else:
                st.warning("âš ï¸ é€‰é¡¹åˆ†å¸ƒä¸å¤Ÿå‡åŒ€ï¼Œæœ€å¤§åå·®è¶…è¿‡5%ï¼Œå»ºè®®é™ä½AIåˆ›é€ æ€§å€¼é‡æ–°ç”Ÿæˆ")
                
            # æ£€æŸ¥Bé€‰é¡¹æ¯”ä¾‹
            if 'B' in answer_counts and answer_counts['B']/total*100 > 30:
                st.error("âŒ Bé€‰é¡¹æ¯”ä¾‹è¿‡é«˜ï¼Œè¶…è¿‡30%ï¼Œå»ºè®®é™ä½AIåˆ›é€ æ€§å€¼é‡æ–°ç”Ÿæˆ")
        else:
            st.info("æœªæ‰¾åˆ°é€‰æ‹©é¢˜ç­”æ¡ˆæ•°æ®")
    
    with tab2:
        st.subheader("ğŸ“š å•è¯åˆ†å¸ƒåˆ†æ")
        
        # æå–è¯•å·ä¸­ä½¿ç”¨çš„å•è¯
        if word_data and 'words' in word_data:
            # ä»JSONæ•°æ®ä¸­è·å–å®Œæ•´å•è¯åˆ—è¡¨
            all_words = [w['word'].lower() for w in word_data['words']]
            total_words = len(all_words)
            
            # ä»è¯•å·ä¸­æå–ä½¿ç”¨çš„å•è¯
            # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼æ¥åŒ¹é…å•è¯ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„åŒ¹é…é€»è¾‘
            word_pattern = re.compile(r'\b([a-zA-Z]+)\b')
            used_words_raw = word_pattern.findall(markdown_text.lower())
            
            # è¿‡æ»¤å‡ºè¯åº“ä¸­çš„å•è¯
            used_words = [w for w in used_words_raw if w in all_words]
            used_words_set = set(used_words)
            
            # è®¡ç®—å•è¯ä½¿ç”¨æƒ…å†µ
            front_cutoff = int(total_words * 0.2)  # å‰20%
            back_start = int(total_words * 0.5)    # å50%
            
            front_words = set(all_words[:front_cutoff])
            middle_words = set(all_words[front_cutoff:back_start])
            back_words = set(all_words[back_start:])
            
            front_used = front_words.intersection(used_words_set)
            middle_used = middle_words.intersection(used_words_set)
            back_used = back_words.intersection(used_words_set)
            
            # è®¡ç®—å„éƒ¨åˆ†ä½¿ç”¨æ¯”ä¾‹
            front_ratio = len(front_used) / len(front_words) if front_words else 0
            middle_ratio = len(middle_used) / len(middle_words) if middle_words else 0
            back_ratio = len(back_used) / len(back_words) if back_words else 0
            
            # åˆ›å»ºå•è¯åˆ†å¸ƒè¡¨æ ¼
            word_data = {
                'è¯åº“éƒ¨åˆ†': ['å‰20%å•è¯', 'ä¸­é—´30%å•è¯', 'å50%å•è¯'],
                'ä½¿ç”¨æ•°é‡': [len(front_used), len(middle_used), len(back_used)],
                'æ€»æ•°é‡': [len(front_words), len(middle_words), len(back_words)],
                'ä½¿ç”¨æ¯”ä¾‹': [f"{front_ratio*100:.1f}%", f"{middle_ratio*100:.1f}%", f"{back_ratio*100:.1f}%"]
            }
            
            # æ˜¾ç¤ºåˆ†å¸ƒè¡¨æ ¼
            st.dataframe(pd.DataFrame(word_data))
            
            # åˆ›å»ºå•è¯åˆ†å¸ƒå›¾è¡¨
            fig, ax = plt.subplots(figsize=(10, 6))
            bars = ax.bar(
                word_data['è¯åº“éƒ¨åˆ†'],
                [front_ratio*100, middle_ratio*100, back_ratio*100],
                color=['#3498db', '#2ecc71', '#e74c3c']
            )
            
            # æ·»åŠ ç™¾åˆ†æ¯”æ ‡ç­¾
            for bar in bars:
                height = bar.get_height()
                ax.text(
                    bar.get_x() + bar.get_width()/2.,
                    height + 0.5,
                    f'{height:.1f}%',
                    ha='center', 
                    va='bottom'
                )
            
            # æ·»åŠ ç›®æ ‡åˆ†å¸ƒå‚è€ƒçº¿
            ax.axhline(y=30, color='r', linestyle='--', alpha=0.3)
            ax.text(0, 31, 'å‰20%ç›®æ ‡ (â‰¤30%)', color='r', alpha=0.7)
            
            ax.axhline(y=40, color='g', linestyle='--', alpha=0.3)
            ax.text(2, 41, 'å50%ç›®æ ‡ (â‰¥40%)', color='g', alpha=0.7)
            
            # è®¾ç½®å›¾è¡¨æ ·å¼
            ax.set_ylim(0, max([front_ratio*100, middle_ratio*100, back_ratio*100]) + 10)
            ax.set_ylabel('ä½¿ç”¨æ¯”ä¾‹ (%)')
            ax.set_title('å•è¯åˆ†å¸ƒæƒ…å†µ')
            ax.grid(axis='y', alpha=0.3)
            
            # æ˜¾ç¤ºå›¾è¡¨
            st.pyplot(fig)
            
            # åˆ†æç»“æœ
            if front_ratio <= 0.3 and back_ratio >= 0.4:
                st.success("âœ… å•è¯åˆ†å¸ƒç¬¦åˆè¦æ±‚ï¼šå‰20%å•è¯ä½¿ç”¨ç‡â‰¤30%ï¼Œå50%å•è¯ä½¿ç”¨ç‡â‰¥40%")
            elif front_ratio > 0.3:
                st.warning(f"âš ï¸ å‰20%å•è¯ä½¿ç”¨ç‡è¿‡é«˜ ({front_ratio*100:.1f}% > 30%)ï¼Œå»ºè®®é™ä½AIåˆ›é€ æ€§å€¼é‡æ–°ç”Ÿæˆ")
            elif back_ratio < 0.4:
                st.warning(f"âš ï¸ å50%å•è¯ä½¿ç”¨ç‡è¿‡ä½ ({back_ratio*100:.1f}% < 40%)ï¼Œå»ºè®®é™ä½AIåˆ›é€ æ€§å€¼é‡æ–°ç”Ÿæˆ")
        else:
            st.info("æœªæ‰¾åˆ°å•è¯æ•°æ®ï¼Œæ— æ³•åˆ†æå•è¯åˆ†å¸ƒæƒ…å†µ")


st.set_page_config(page_title="ğŸ“˜ AIè¯æ±‡æµ‹è¯•å·ç”Ÿæˆå™¨", layout="wide")
st.title("ğŸ“˜ AIè¯æ±‡æµ‹è¯•å·ç”Ÿæˆå™¨")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("âš™ï¸ ç”Ÿæˆè®¾ç½®")
    temperature = st.slider(
        "AIåˆ›é€ æ€§", 
        min_value=0.1, 
        max_value=1.0, 
        value=0.3, 
        step=0.1,
        help="è¾ƒä½çš„å€¼(0.1-0.3)ä¼šè®©AIæ›´ä¸¥æ ¼éµå¾ªæŒ‡ä»¤ï¼Œè¾ƒé«˜çš„å€¼ä¼šå¢åŠ åˆ›é€ æ€§ä½†å¯èƒ½å¯¼è‡´ä¸éµå¾ªé¢„é€‰å•è¯"
    )
    st.markdown("""---
    **æ–°ç‰¹æ€§**: ç°åœ¨ä½¿ç”¨ç¨‹åºéšæœºé€‰æ‹©å•è¯å’Œåˆ†é…é€‰é¡¹ï¼Œä¸å†ä¾èµ–AIéšæœºï¼Œç¡®ä¿çœŸæ­£çš„éšæœºåˆ†å¸ƒã€‚
    """)
    st.markdown("""---
    **æç¤º**: å•è¯é€‰æ‹©å’Œé€‰é¡¹åˆ†å¸ƒå·²ç”±ç¨‹åºä¿è¯ï¼Œä½†å¦‚æœAIä¸éµå¾ªé¢„é€‰å•è¯ï¼Œè¯·é™ä½AIåˆ›é€ æ€§å€¼é‡è¯•ã€‚
    """)
    st.markdown("""---
    **ç‰ˆæœ¬**: 2.0.0 (ç®€åŒ–ç‰ˆç¨‹åºéšæœºé€‰è¯)
    """)


st.markdown("### ğŸ§¾ ç¬¬ä¸€æ­¥ï¼šä¸Šä¼ ä½ çš„ JSON æ–‡ä»¶ï¼ˆä»»é€‰å…¶ä¸€ï¼‰")
col1, col2 = st.columns(2)

with col1:
    raw_file = st.file_uploader("ğŸ“¥ ä¸Šä¼ åŸå§‹è¯æ±‡æ–‡ä»¶ï¼ˆä»…åŒ…å« 'words': [...]ï¼‰", type="json", key="raw")
with col2:
    enhanced_file = st.file_uploader("ğŸ“¥ ä¸Šä¼ å¢å¼ºåè¯æ±‡æ–‡ä»¶ï¼ˆå·²å«é‡Šä¹‰/ä¾‹å¥ç­‰ï¼‰", type="json", key="enhanced")

def prepare_exam_prompt(enhanced_data, temperature):
    """
    å‡†å¤‡è¯•å·ç”Ÿæˆçš„æç¤ºè¯ï¼ŒåŒ…å«é¢„å…ˆéšæœºé€‰æ‹©çš„å•è¯
    """
    import random
    
    # è·å–æ‰€æœ‰å•è¯åˆ—è¡¨
    all_words = enhanced_data["words"]
    word_list = [w["word"] for w in all_words]
    
    # å®šä¹‰å„é¢˜å‹éœ€è¦çš„å•è¯æ•°é‡
    section_counts = {
        "è‹±è¯‘ä¸­": 15,
        "ä¸­è¯‘è‹±": 10,
        "é€‰è¯å¡«ç©º": 10,
        "è¯ä¹‰è¾¨æ": 5,
        "ç¿»è¯‘å¥å­": 5,
        "çŸ­æ–‡å¡«ç©º": 10,
        "è¯å½¢å˜åŒ–": 10
    }
    
    # éšæœºé€‰æ‹©å„é¢˜å‹çš„å•è¯
    used_words = set()
    section_words = {}
    
    # ä¸ºæ¯ä¸ªé¢˜å‹é€‰æ‹©å•è¯
    for section, count in section_counts.items():
        # è¿‡æ»¤æ‰å·²ä½¿ç”¨çš„å•è¯
        available_words = [w for w in word_list if w not in used_words]
        
        # å¦‚æœå¯ç”¨å•è¯ä¸è¶³ï¼Œåˆ™é‡ç½®å·²ä½¿ç”¨å•è¯é›†åˆï¼ˆçŸ­æ–‡å¡«ç©ºå¯ä»¥å¤ç”¨å•è¯ï¼‰
        if len(available_words) < count and section != "çŸ­æ–‡å¡«ç©º":
            st.warning(f"âš ï¸ å¯ç”¨å•è¯ä¸è¶³ï¼Œéƒ¨åˆ†å•è¯å°†åœ¨å¤šä¸ªé¢˜å‹ä¸­é‡å¤ä½¿ç”¨")
            # åªä¿ç•™å½“å‰é¢˜å‹ä¹‹å‰çš„å·²ä½¿ç”¨å•è¯
            used_words = set()
            available_words = word_list
        
        # ç®€å•éšæœºé€‰æ‹©å•è¯
        selected = random.sample(available_words, min(count, len(available_words)))
        
        # è®°å½•é€‰æ‹©çš„å•è¯
        section_words[section] = selected
        
        # æ›´æ–°å·²ä½¿ç”¨å•è¯é›†åˆï¼ˆçŸ­æ–‡å¡«ç©ºå¯ä»¥å¤ç”¨å•è¯ï¼‰
        if section != "çŸ­æ–‡å¡«ç©º":
            used_words.update(selected)
    
    # ä¸ºé€‰æ‹©é¢˜ç”Ÿæˆå‡åŒ€åˆ†å¸ƒçš„ç­”æ¡ˆä½ç½®
    answer_positions = {}
    
    # ä¸ºè‹±è¯‘ä¸­é¢˜å‹åˆ†é…ç­”æ¡ˆä½ç½®
    positions_è‹±è¯‘ä¸­ = ['A', 'B', 'C', 'D'] * (section_counts["è‹±è¯‘ä¸­"] // 4)
    if section_counts["è‹±è¯‘ä¸­"] % 4 > 0:
        positions_è‹±è¯‘ä¸­.extend(['A', 'B', 'C', 'D'][:section_counts["è‹±è¯‘ä¸­"] % 4])
    random.shuffle(positions_è‹±è¯‘ä¸­)
    answer_positions["è‹±è¯‘ä¸­"] = positions_è‹±è¯‘ä¸­
    
    # ä¸ºè¯ä¹‰è¾¨æé¢˜å‹åˆ†é…ç­”æ¡ˆä½ç½®
    positions_è¯ä¹‰è¾¨æ = ['A', 'B', 'C', 'D'] * (section_counts["è¯ä¹‰è¾¨æ"] // 4)
    if section_counts["è¯ä¹‰è¾¨æ"] % 4 > 0:
        positions_è¯ä¹‰è¾¨æ.extend(['A', 'B', 'C', 'D'][:section_counts["è¯ä¹‰è¾¨æ"] % 4])
    random.shuffle(positions_è¯ä¹‰è¾¨æ)
    answer_positions["è¯ä¹‰è¾¨æ"] = positions_è¯ä¹‰è¾¨æ
    
    # æ„å»ºåŒ…å«é¢„é€‰å•è¯çš„æç¤ºè¯
    prompt_template = load_prompt("templates/exam_prompt.txt")
    
    # æ·»åŠ é¢„é€‰å•è¯å’Œç­”æ¡ˆä½ç½®ä¿¡æ¯
    prompt_with_selections = prompt_template.replace(
        "{ç²˜è´´ä½ çš„å®Œæ•´ JSON æ•°æ®åœ¨è¿™é‡Œ}", 
        json.dumps(enhanced_data, ensure_ascii=False)
    )
    
    # æ·»åŠ é¢„é€‰å•è¯ä¿¡æ¯
    prompt_with_selections += "\n\n# é¢„é€‰å•è¯ï¼ˆå¿…é¡»ä¸¥æ ¼ä½¿ç”¨ï¼‰\n"
    for section, words in section_words.items():
        prompt_with_selections += f"## {section}é¢˜å‹å•è¯\n"
        prompt_with_selections += ", ".join(words) + "\n\n"
    
    # æ·»åŠ é¢„åˆ†é…ç­”æ¡ˆä½ç½®ä¿¡æ¯
    prompt_with_selections += "\n# é¢„åˆ†é…ç­”æ¡ˆä½ç½®ï¼ˆå¿…é¡»ä¸¥æ ¼æŒ‰é¡ºåºä½¿ç”¨ï¼‰\n"
    for section, positions in answer_positions.items():
        prompt_with_selections += f"## {section}é¢˜å‹ç­”æ¡ˆä½ç½®\n"
        prompt_with_selections += ", ".join(positions) + "\n\n"
    
    return prompt_with_selections

# å¦‚æœä¸Šä¼ çš„æ˜¯åŸå§‹ JSONï¼ˆä»…æœ‰ wordsï¼‰
if raw_file:
    raw_data = json.load(raw_file)
    if "words" in raw_data:
        st.success(f"âœ… åŸå§‹è¯æ±‡åˆ—è¡¨è¯»å–æˆåŠŸï¼Œå…± {len(raw_data['words'])} ä¸ªå•è¯")

        if st.button("ğŸš€ ç”Ÿæˆå¢å¼ºè¯æ±‡ä¿¡æ¯"):
            enhance_prompt = load_prompt("templates/enhance_prompt.txt").replace(
                "{å•è¯åˆ—è¡¨}", json.dumps(raw_data["words"], ensure_ascii=False)
            )
            st.info("â³ AIæ­£åœ¨ç”Ÿæˆç»“æ„åŒ–è¯æ±‡ä¿¡æ¯...")
            enhanced_json_str = call_deepseek_api(enhance_prompt, temperature=0.7)

            # è§£æå¢å¼ºåçš„JSON
            try:
                enhanced_data = json.loads(enhanced_json_str)
                st.code(enhanced_json_str, language="json")
                st.download_button("ğŸ“¥ ä¸‹è½½å¢å¼ºç‰ˆJSON", enhanced_json_str, file_name="enhanced_words.json")

                # è‡ªåŠ¨ä¼ é€’ç»™è¯•å·ç”Ÿæˆæ¨¡å—
                with st.expander("ğŸ‘‰ ç»§ç»­ç”Ÿæˆè¯•å·", expanded=True):
                    if st.button("ğŸ“ ç”Ÿæˆè¯•å·"):
                        # ä½¿ç”¨ç¨‹åºéšæœºé€‰æ‹©å•è¯å¹¶å‡†å¤‡æç¤ºè¯
                        exam_prompt = prepare_exam_prompt(enhanced_data, temperature)
                        st.info("â³ AIæ­£åœ¨ç”Ÿæˆæµ‹è¯•å·...")
                        markdown_exam = call_deepseek_api(exam_prompt, temperature=temperature)
                        st.markdown(markdown_exam)
                        st.download_button("ğŸ“¥ ä¸‹è½½è¯•å· Markdown", markdown_exam, file_name="word_test.md")
                        
                        # æ·»åŠ åˆ†å¸ƒåˆ†æåŠŸèƒ½
                        with st.expander("ğŸ“Š æŸ¥çœ‹åˆ†å¸ƒåˆ†æ", expanded=False):
                            analyze_distribution(markdown_exam, enhanced_data)
            except json.JSONDecodeError:
                st.error("âŒ ç”Ÿæˆçš„JSONæ ¼å¼æœ‰è¯¯ï¼Œè¯·é‡è¯•")
                st.code(enhanced_json_str, language="json")

# å¦‚æœç”¨æˆ·ç›´æ¥ä¸Šä¼ äº†å¢å¼ºç‰ˆ JSONï¼ˆè·³è¿‡ç¬¬ä¸€æ­¥ï¼‰
elif enhanced_file:
    enhanced_data = json.load(enhanced_file)
    st.success("âœ… å¢å¼ºè¯æ±‡æ•°æ®è¯»å–æˆåŠŸï¼Œå¯ç›´æ¥ç”Ÿæˆè¯•å·")

    if st.button("ğŸ“ ç›´æ¥ç”Ÿæˆè¯•å·"):
        # ä½¿ç”¨ç¨‹åºéšæœºé€‰æ‹©å•è¯å¹¶å‡†å¤‡æç¤ºè¯
        exam_prompt = prepare_exam_prompt(enhanced_data, temperature)
        st.info("â³ AIæ­£åœ¨ç”Ÿæˆæµ‹è¯•å·...")
        markdown_exam = call_deepseek_api(exam_prompt, temperature=temperature)
        st.markdown(markdown_exam)
        st.download_button("ğŸ“¥ ä¸‹è½½è¯•å· Markdown", markdown_exam, file_name="word_test.md")
        
        # æ·»åŠ åˆ†å¸ƒåˆ†æåŠŸèƒ½
        with st.expander("ğŸ“Š æŸ¥çœ‹åˆ†å¸ƒåˆ†æ", expanded=False):
            analyze_distribution(markdown_exam, enhanced_data)
